---
layout: home
order: 1
permalink: /
title: Workshop 2025
# redirect_from: /index.html
desc_title: Data in Generative Models @ ICML 2025
description: <strong>The Bad, the Ugly, and the Greats</strong> - Building reliable and trustworthy Generative AI must start with high-quality and responsibly managed training data.
social: true
---

<!-- <td style="text-align:center"><img src="assets/img/workshop-votes.png" height="170"></td> <br />
<td style="text-align:center"><a href="https://bit.ly/bugs-orals">Vote Best Oral</a> | <a href="https://bit.ly/bugs-posters">Vote Best Poster</a></td> <br /> -->

Generative models have become extremely powerful and are now integral to various aspects of daily life from creative arts to customer service. Given their increasing interaction with people, ensuring their trustworthiness is crucial. This workshop centers on the idea that the safety and reliability of generative models are deeply connected to the nature and treatment of their training data. We aim to explore the hypothesis that building reliable and trustworthy artificial intelligence (AI) systems based on generative models must start with high-quality and responsibly managed data.

The workshop will focus on several key areas where training data impacts the trustworthiness of generative models. Among others, we will address 1) **privacy** concerns, highlighting how improper inclusion and handling of sensitive information in the training data can lead to significant privacy violations; 2) **safety** risks, like backdoors and data poisoning that threaten robust generations; and 3) the impact of **biases** in generative models' training data, which can cause models to perpetuate or even amplify societal biases, resulting in unfair outcomes.

Through expert talks, panel discussions, and interactive sessions, participants will delve into these issues and explore strategies for developing safe, trustworthy, and reliable generative models. This workshop aims to foster collaboration and drive forward research to ensure that generative models, as they become more embedded in our lives, do so in a trustworthy and beneficial manner.

<!-- **UPDATE**: fill out this form if you are interested in a post-workshop social: [https://forms.gle/XjeSVmyHnsp7EmLB6](https://forms.gle/XjeSVmyHnsp7EmLB6). -->

<!-- ### Schedule (Meeting Room 317A, 9 AM - 5 PM, July 29, 2023) -->
### Schedule

<!-- ⭐ **Link to NeurIPS page: [https://neurips.cc/virtual/2023/workshop/66550](https://neurips.cc/virtual/2023/workshop/66550)** ⭐ -->
#### ⭐ **Coming Soon** ⭐


<!-- |----------------------|---------------------------------------------------------|---------------------------------------------------------------------------------------|
| Start Time (CST/GMT-06:00, New Orleans)  |  Session                                                 | Speaker(s)                                                                            |
|:---------------------|:--------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|
| 08:55 am | Opening Remarks                                                                            | Organizers                                                                            |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 09:00 am | **Invited Talk 1:** A Blessing in Disguise: Backdoor Attacks as Watermarks for Dataset Copyright | Yiming Li |
| 09:30 am | **Invited Talk 2:** Recent Advances in Backdoor Defense and Benchmark | Baoyuan Wu  |
| 10:00 am | Coffee Break                                                                           |  |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 10:30 am | **Invited Talk 3:** The difference between safety and security for watermarking                                                                                | Jonas Geiping |
| 11:00 am | **Oral 1:** Effective Backdoor Mitigation Depends on the Pre-training Objective | Sahil Verma, Gantavya Bhatt, Soumye Singhal, Arnav Das, Chirag Shah, John Dickerson, Jeff A Bilmes |
| 11:15 am  | **Invited Talk 4:** Universal jailbreak backdoors from poisoned human feedback | Florian Tramèr |
| 11:45 am | Lunch Break | |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 01:00 pm | **Oral 2:** VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models | Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho |
| 01:15 pm | **Oral 3:** The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline | Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi |
| 01:30 pm | **Invited talk 5:** Is this model mine? On stealing and defending machine learning models | Adam Dziedzic |
| 02:00 pm | **Invited talk 6**                                                                           | Ruoxi Jia |
| 02:30 pm | Coffee Break                                                                     |  |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 03:00 pm | **Poster Session**                                                                                | Paper Authors |
| 03:45 pm | **Oral 4:** Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection | Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin |
| 04:00 pm | **Oral 5:** BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models | Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li |
| 04:15 pm | **Invited Talk 7:** Decoding Backdoors in LLMs and Their Implications | Bo Li |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 04:45 pm | **Panel Discussion**                                                                     | Moderator: Eugene Bagdasaryan |
| 05:15 pm   | Closing Remarks                                                                        | Organizers    |  -->


<!-- ### Speakers (TBD) -->

### Speakers (Updating)

<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/pinyuchen-square.webp" height="170" width="170"></td>
    <!-- <td style="text-align:center"><img src="assets/img/icml2025/speakers/zicokolter-square.jpg" height="170" width="170"></td> -->
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/ericwallace-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/sanmikoyejo-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://sites.google.com/site/pinyuchenpage/home">Pin-Yu Chen</a> <small> <br> Principal Research Scientist <br> IBM Research AI </small> </td>
    <!-- <td style="text-align:center"><a href="https://zicokolter.com/">Zico Kolter</a> <small> <br> Professor <br> Carnegie Mellon University </small> </td> -->
    <td style="text-align:center"><a href="https://www.ericswallace.com/">Eric Wallace</a> <small> <br> Member of Technical Staff <br> OpenAI </small> </td>
    <td style="text-align:center"><a href="https://cs.stanford.edu/~sanmi/">Sanmi Koyejo</a> <small><br> Assistant Professor <br> Stanford University </small></td>
  </tr>
  <tr>
    <!-- <td style="text-align:center"><img src="assets/img/icml2025/speakers/dawnsong-square.jpg" height="170" width="170"></td> -->
    
    <td style="text-align:center"><img src="assets/img/icml2025/speakers/nouhadziri-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://riceric22.github.io/">Nouha Dziri</a> <small> <br> Research Scientist <br> Allen Institute for AI (AI2) </small> </td>
    <!-- <td style="text-align:center"><a href="https://dawnsong.io/">Dawn Song</a> <small> <br> Professor <br> University of California, Berkeley	</small> </td> -->
    <!-- <td style="text-align:center"><a href="https://riceric22.github.io/">Eric Wong</a> <small> <br> Assistant Professor <br> University of Pennsylvania </small> </td> -->
  </tr>
</table>

### Panelists (TBD)

<!-- ### Panelists (Tentative) -->

<!-- 
<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/tatsunorihashimoto-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/adinawilliams-square.jpg" height="170" width="170"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/panelists/rexying-square.jpg" height="170" width="170"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://thashim.github.io/">Tatsunori Hashimoto</a> <small> <br>Assistant Professor <br> Stanford University </small> </td>
    <td style="text-align:center"><a href="https://ai.meta.com/people/1396973444287406/adina-williams/">Adina Williams</a> <small> <br>Research Scientist <br> Facebook AI Research </small></td>
    <td style="text-align:center"><a href="https://www.cs.yale.edu/homes/ying-rex/">Rex (Zhitao) Ying</a> <small> <br>Assistant Professor <br> Yale University </small> </td>
  </tr>
</table> -->

### Call for Papers

**We cordially invite submissions and participation in our “Data in Generative Models (The Bad, the Ugly, and the Greats)” workshop that will be held on July 18th or July 19th, 2025 at the Forty-Second International Conference on Machine Learning (ICML) 2025 in Vancouver, Canada.**

<!-- The submission deadline is **<s>September 29, 2023</s> October 6th, 2023, 23:59 AoE** and the submission link <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>. -->

#### Motivation and Topics

Building upon the success of our previous workshop, <a href="https://neurips2023-bugs.github.io/">BUGS@NeurIPS 2023</a>, we will continue to explore the impact of data on AI *beyond backdoor attacks*. A key enhancement of the new 2025 edition of the workshop is the substantial expansion of the 2023 research themes, focusing on the interaction between Datasets and Generative Models, including large language models, diffusion models, and vision-language models. Examples of research areas include: 
* **Data-Centric Approach to the Safety of Generative Models (general theme):** Most research in safe machine learning concentrates primarily on evaluating model properties [(Papernot et. al.)] [ref1]. However, from a data-centric perspective, we focus on the safety of generative models, such as LLMs and diffusion models. This strategy will create new methodologies and reveal vulnerabilities and potential threats that have not been previously sufficiently recognized by the research community.


* **Data Memorization in Generative Models:** Current approaches to prevent data memorization in generative models propose limited solutions, such as removing, collecting, or augmenting specific data samples [(Wen et. al.)] [ref2]. This area of research aims to achieve a comprehensive understanding of data memorization, extending the analysis beyond training set distributions and model outputs. This requires us to examine the characteristics of individual data samples that make them susceptible to memorization, focusing on their representations across multiple internal model layers rather than solely on model outputs, as done in previous studies.

* **Data Contamination in Generative Models:** Including test data from downstream tasks and benchmarks in the training data of generative models poses a significant challenge in accurately measuring the models' true effectiveness. Developing new, effective methods for detecting data contamination within generative models is essential. These innovative approaches should focus on identifying potential contamination at the individual instance level and then use this information to evaluate broader contamination at the dataset level.

* **Data Verification in Generative Models:**This area focuses on the inputs and outputs of generative models. Current methods for identifying training samples and verifying model outputs are typically assessed using academic benchmark datasets and small-scale models, as highlighted in the recent work ([Dubiński et. al.] [ref3], [Duan et. al.] [ref4]). We aim to bridge the gap by focusing on data verification for large-scale generative models trained on extensive datasets, ensuring privacy protection on a large scale in real-world scenarios. 


We welcome submissions related to all aspects of Data in Generative AI, including but not limited to: 

* Data-Centric Approach to the Safety of Generative Models
* Data Memorization in Generative Models
* Data Contamination in Generative Models
* Data Verification in Generative Models
* Data Poisoning and Backdoors in Generative Models
* Generative Data for Trustworthy AI Research (e.g., synthetic datasets for security studies, synthetic augmentation for robust models, etc.)

The workshop will employ a double-blind review process. Each submission will be evaluated based on the following criteria:

* Soundness of the methodology
* Relevance to the workshop
* Societal impacts

We only consider submissions that haven’t been published in any peer-reviewed venue, including ICML 2025 conference. **We allow dual submissions with other workshops or conferences. The workshop is non-archival and will not have any official proceedings**. All accepted papers will be allocated either a poster presentation or a talk slot.
 
<!-- ### Call for Reviewers
Please fill out this [Google form](https://docs.google.com/forms/d/e/1FAIpQLSd3L9_o7vAZUSWjWMxi18jZHuIrBaafUBm6v1fTZQorK2o9Qw/viewform) if you are interested in reviewing for the workshop.

🏆 **2 free ICML 2023 workshop registrations will be given as "Best Reviewer Awards"** 🏆 -->

### Important Dates (Tentative)

* **Submission deadline**: May 20th, 2025, 11:59 PM Anywhere on Earth (AoE)
* **Author notification**: June 9th, 2025
* **Camera-ready deadline**: June 30th, 2025 11:59 PM Anywhere on Earth (AoE)
* **Workshop date**: TBD (Full-day Event)

### Submission Instructions
Papers should be submitted to OpenReview: https://openreview.net/group?id=ICML.cc/2025/Workshop/DIG-BUG

We welcome both **long** (up to 6 pages) and **short/position** (up to 4 pages) papers, excluding references, acknowledgments, or appendices. Please use the submission template provided at https://media.icml.cc/Conferences/ICML2025/Styles/icml2025.zip. Submissions must be anonymous following ICML double-blind reviewing guidelines, ICML Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows.

<!-- Papers should be submitted to OpenReview: <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>

Submitted papers should have up to 6 pages (excluding references, acknowledgments, or appendices). Please use the NeurIPS submission template provided at <a href="https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles">https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles</a>.
Submissions must be anonymous following NeurIPS double-blind reviewing guidelines, NeurIPS Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows. -->

### Workshop Sponsors

#### ⭐ **Please reach out if you would like to sponsor our workshop.** ⭐

<!-- <table style="width:100%; border: none;">
<td style="text-align:center; border: none;"><a href="https://troj.ai/"><img src="assets/img/sponsor-troj-ai.png" height="55"></a></td>

<td style="text-align:center; border: none;"><a href="https://ml.umd.edu/"><img src="assets/img/sponsor-umd-cml.png" height="65"></a></td>

<td style="text-align:center; border: none;"><a href="https://www.google.org/"><img src="assets/img/sponsor-google.png" height="75"></a></td>
</table> -->

### Organizers 


<table style="width:100%">
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/franziskaboenisch-square.jpg" height="150"  width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/khoadoan-square.jpg" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/adamdziedzic-square.png" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/boli-square.png" height="150" width="150"></td>
    </tr>
  <tr>
    <td style="text-align:center"><a href="https://cispa.de/en/people/c01frbo">Franziska Boenisch</a> <br> <small> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://mail-research.com/">Khoa D Doan</a> <small> <br> CECS-VinUniversity & VinUni-Illinois Smart Health Center </small> </td>
    <td style="text-align:center"><a href="https://adam-dziedzic.com/">Adam Dziedzic</a> <small> <br> CISPA Helmholtz Center for Information Security </small> </td>
    <td style="text-align:center"><a href="https://aisecure.github.io/">Bo Li</a> <small> <br>University of Illinois at Urbana-Champaign </small> </td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/vietanhnguyen-square.jpg" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/aniruddhasaha-square.jpeg" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/zhenting-wang-square.jpg" height="150" width="150"></td>
    <td style="text-align:center"><img src="assets/img/icml2025/organizers/heatherzheng-square.jpg" height="150" width="150"></td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://www.vietanhnguyen.net/">Viet Anh Nguyen</a> <small> <br>Chinese University of Hong Kong </small> </td>
    <td style="text-align:center"><a href="https://ani0075saha.github.io/">Aniruddha Saha</a> <small> <br>Independent Researcher </small> </td>
    <td style="text-align:center"><a href="https://zhentingwang.github.io/">Zhenting Wang</a> <small> <br>Rutgers University </small> </td>
    <td style="text-align:center"><a href="https://people.cs.uchicago.edu/~htzheng/">Heather Zheng</a> <small> <br>University of Chicago </small> </td>
  </tr>
</table>


### Organizer affiliations

<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://cispa.de/en"><img src="assets/img/icml2025/organizers/affiliations/cispa.png" height="50"></a></td>    
    <td style="text-align:center; border: none; spacing: none"><a href="https://illinois.edu/"><img src="assets/img/icml2025/organizers/affiliations/uiuc.png" height="50"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.rutgers.edu/"><img src="assets/img/icml2025/organizers/affiliations/rutgers.png" height="50"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.uic.edu/"><img src="assets/img/icml2025/organizers/affiliations/uic.png" height="60"></a></td>
  </tr>
</table>
<table style="width:100%; align: left; border: none; spacing: none">
  <tr> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://cecs.vinuni.edu.vn/"><img src="assets/img/icml2025/organizers/affiliations/vinuni.png" height="50"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://smarthealth.vinuni.edu.vn/"><img src="assets/img/icml2025/organizers/affiliations/shc.png" height="50"></a></td>    
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.cuhk.edu.hk/english/index.html"><img src="assets/img/icml2025/organizers/affiliations/cuhk.png" height="50"></a></td>
  </tr>
</table>

[ref1]: https://ieeexplore.ieee.org/document/8406613 "Nicolas Papernot et al. Sok: Security and privacy in machine learning. In EuroS&P, pages 399–414. IEEE, 2018."
[ref2]: https://openreview.net/forum?id=84n3UwkH7b "Wen, Yuxin, Yuchen Liu, Chen Chen, and Lingjuan Lyu. "Detecting, explaining, and mitigating memorization in diffusion models." In The Twelfth International Conference on Learning Representations. 2024."
[ref3]: https://openaccess.thecvf.com/content/WACV2024/html/Dubinski_Towards_More_Realistic_Membership_Inference_Attacks_on_Large_Diffusion_Models_WACV_2024_paper.html "Dubiński, Jan, Antoni Kowalczuk, Stanisław Pawlak, Przemyslaw Rokita, Tomasz Trzciński, and Paweł Morawiecki. "Towards more realistic membership inference attacks on large diffusion models." In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 4860-4869. 2024."
[ref4]: https://arxiv.org/abs/2402.07841 "Duan, Michael, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. "Do membership inference attacks work on large language models?." arXiv preprint arXiv:2402.07841 (2024)."